{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['SPARK_HOME']=r'C:/spark/'\n",
    "os.environ['HADOOP_HOME'] = r'C:/hadoop/'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='lab'\n",
    "os.environ['PYSPARK_PYTHON']='python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark datasets**\n",
    "\n",
    "1. RDD\n",
    "- Low Level API\n",
    "- Funcionales\n",
    "\n",
    "2. DataFrames\n",
    "- High Level API\n",
    "- Relacional\n",
    "- Optimizados para querys\n",
    "\n",
    "**Mapper Transformations**\n",
    "\n",
    "1. map(f) >> Relacion 1 a 1 >> aplica la funcion f() a un RDD\n",
    "2. mapValues(f) >> Relacion 1 a 1 >> pasa cada pareja (key,value) del RDD a f()\n",
    "3. flatMap(f) >> Relacion 1 a muchos >> Aplica la funcion f() a los elementos del RDD y aplana los resultados\n",
    "4. flatMapValues(f) >> Relacion 1 a muchos >> Pasa cada valor (key,value) del RDD por el flatMap(f) sin cambiar keys\n",
    "5. mapPartitions(f) >> Relacion muchos a 1 >> Devuelve un RDD aplicando la funcion f() a cada particion del RDD fuente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. map()\n",
    "\n",
    "- Es la más común y se puede aplicar a RDD o Dataframes\n",
    "- Se aplica la ufncion a cada elemento, no es secuencial \n",
    "- El source RDD se particiona para ser procesado independiente y concurrente (e.g si tu dataset tiene 40 billones de elementos se puede particionar en 2 millones de elementos aunque dependera de tu cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_david(x):\n",
    "    if (x> 1):\n",
    "        return x+3\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, -1, 2, -2, 4, 5, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"App1\").getOrCreate()\n",
    "# Data\n",
    "data = [1, 3, -1, 2, -2, 4, 5, 6]\n",
    "# Creamos el RDD\n",
    "rdd = spark.sparkContext.parallelize(data) # RDD[integer]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>App1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f6b8264790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 0, 5, 0, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lambda expression\n",
    "rdd2 = rdd.map(lambda x: map_david(x)) # RDD [Integer]\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 0, 5, 0, 7, 8, 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USando la funcion\n",
    "rdd3 = rdd.map(map_david) # RDD Ìnteger]\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (3, 6), (-1, 0), (2, 5), (-2, 0), (4, 7), (5, 8), (6, 9)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otra forma\n",
    "rdd4 = rdd.map(lambda x: (x, map_david(x))) # RDD [Integer]\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('David', 2), ('Juan', -1), ('Andrea', 5), ('Pedro', -3), ('Sofia', 8)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otro ejemplo\n",
    "spark = SparkSession.builder.appName(\"App2\").getOrCreate()\n",
    "par= [('David',2),('Juan',-1),('Andrea',5),('Pedro',-3),('Sofia',8)]\n",
    "# Este es un RDD [(String, Integer)]\n",
    "rdd = spark.sparkContext.parallelize(par)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('David', 2, 102),\n",
       " ('Juan', -1, 99),\n",
       " ('Andrea', 5, 105),\n",
       " ('Pedro', -3, 97),\n",
       " ('Sofia', 8, 108)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda kv: (kv[0], kv[1], kv[1] + 100))\n",
    "# Esto ahora es un RDD[(String, Integer, Integer)]\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David,10,11', 'Juan,5,4', 'Pedro,4,5', 'Sofia,5,7']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar a si hicieramos esto\n",
    "def crear_parejas(string):\n",
    "    tokens= string.split(\",\")\n",
    "    return (tokens[0],tokens[1],tokens[2])\n",
    "palabras=['David,10,11','Juan,5,4','Pedro,4,5','Sofia,5,7']\n",
    "rdd= spark.sparkContext.parallelize(palabras)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('David', '10', '11'),\n",
       " ('Juan', '5', '4'),\n",
       " ('Pedro', '4', '5'),\n",
       " ('Sofia', '5', '7')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patrones= rdd.map(crear_parejas) # RDD[(String,String,String)]\n",
    "patrones.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 130),\n",
       " (21, 120),\n",
       " (25, 110),\n",
       " (24, 170),\n",
       " (25, 180),\n",
       " (28, 190),\n",
       " (27, 170),\n",
       " (21, 110),\n",
       " (26, 130),\n",
       " (27, 190)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usemos datos reales\n",
    "def parsing_david(registro):\n",
    "    tokens= registro.split(\",\")\n",
    "    # Extraer valores relevantes\n",
    "    edad= int(tokens[2])\n",
    "    amigos= int(tokens[3])\n",
    "    return (edad, amigos)\n",
    "# Definir path\n",
    "path= 'usuarios.txt'\n",
    "app= spark.sparkContext.textFile(path)\n",
    "patrones= app.map(parsing_david)\n",
    "patrones.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, (130, 1)),\n",
       " (21, (120, 1)),\n",
       " (25, (110, 1)),\n",
       " (24, (170, 1)),\n",
       " (25, (180, 1)),\n",
       " (28, (190, 1)),\n",
       " (27, (170, 1)),\n",
       " (21, (110, 1)),\n",
       " (26, (130, 1)),\n",
       " (27, (190, 1))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Algunas operaciones adicionales\n",
    "patrones.mapValues(lambda x: (x,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, (170, 1)),\n",
       " (28, (190, 1)),\n",
       " (26, (130, 1)),\n",
       " (23, (130, 1)),\n",
       " (21, (230, 2)),\n",
       " (25, (290, 2)),\n",
       " (27, (360, 2))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como algunas edades se repiten podemos contar entonces\n",
    "# RDD[(Integer, (Integer,Integer))]\n",
    "totales=patrones.\\\n",
    "    mapValues(lambda x: (x,1)).\\\n",
    "    reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "# Estamos adicionando las tuplas en este caso y agrupando por el primer Integer\n",
    "totales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 170.0),\n",
       " (28, 190.0),\n",
       " (26, 130.0),\n",
       " (23, 130.0),\n",
       " (21, 115.0),\n",
       " (25, 145.0),\n",
       " (27, 180.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora promedios \n",
    "totales.mapValues(lambda x: float(x[0]/float(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+\n",
      "|nombre|monto|educacion|\n",
      "+------+-----+---------+\n",
      "| david|  440|      PHD|\n",
      "|  juan|  420|      PHD|\n",
      "|   bob|  280|       MS|\n",
      "| betsy|  200|       MS|\n",
      "|andrea|  180|       BS|\n",
      "| maria|  100|       BS|\n",
      "+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sobre dataframes\n",
    "# No tienen formalmente la funcion map() pero se puede alcanzar el mismo resultado\n",
    "# de diversas formas con wothColumn() o .drop()\n",
    "tuples3 = [ ('david', 440, 'PHD'), ('juan', 420, 'PHD'),\n",
    "           ('bob', 280, 'MS'), ('betsy', 200, 'MS'),\n",
    "           ('andrea', 180, 'BS'), ('maria', 100, 'BS') ]\n",
    "df = spark.createDataFrame(tuples3, [\"nombre\", \"monto\", \"educacion\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+\n",
      "|nombre|monto|educacion|bono|\n",
      "+------+-----+---------+----+\n",
      "| david|  440|      PHD|44.0|\n",
      "|  juan|  420|      PHD|42.0|\n",
      "|   bob|  280|       MS|28.0|\n",
      "| betsy|  200|       MS|20.0|\n",
      "|andrea|  180|       BS|18.0|\n",
      "| maria|  100|       BS|10.0|\n",
      "+------+-----+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que tenemos que calcular el bono a pagar\n",
    "# Corresponde al 10% del salario \n",
    "df.rdd.\\\n",
    "    map(lambda x: (x['nombre'],x['monto'],\n",
    "                   x['educacion'],int(x['monto'])/10)).\\\n",
    "    toDF(['nombre','monto','educacion','bono']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+\n",
      "|nombre|monto|educacion|bono|\n",
      "+------+-----+---------+----+\n",
      "| david|  440|      PHD|44.0|\n",
      "|  juan|  420|      PHD|42.0|\n",
      "|   bob|  280|       MS|28.0|\n",
      "| betsy|  200|       MS|20.0|\n",
      "|andrea|  180|       BS|18.0|\n",
      "| maria|  100|       BS|10.0|\n",
      "+------+-----+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Si hay muchas columnas que pasa? \n",
    "df.rdd.\\\n",
    "    map(lambda x: x +\\\n",
    "        (str(x['monto']/10),)).\\\n",
    "    toDF(df.columns +['bono']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (david, 440, PHD, 44.0)\n",
       "1     (juan, 420, PHD, 42.0)\n",
       "2       (bob, 280, MS, 28.0)\n",
       "3     (betsy, 200, MS, 20.0)\n",
       "4    (andrea, 180, BS, 18.0)\n",
       "5     (maria, 100, BS, 10.0)\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "p=pd.DataFrame(tuples3,columns=['nombre','monto','educacion'])\n",
    "p.apply(lambda row: tuple(list(row) + [str(row['monto'] / 10)]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+\n",
      "|nombre|monto|educacion|bono|\n",
      "+------+-----+---------+----+\n",
      "| david|  440|      PHD|44.0|\n",
      "|  juan|  420|      PHD|42.0|\n",
      "|   bob|  280|       MS|28.0|\n",
      "| betsy|  200|       MS|20.0|\n",
      "|andrea|  180|       BS|18.0|\n",
      "| maria|  100|       BS|10.0|\n",
      "+------+-----+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Crear la spark session\n",
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "# Aplicando el WithColumn\n",
    "df.withColumn(\"bono\", lit(df[\"monto\"] / 10)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
