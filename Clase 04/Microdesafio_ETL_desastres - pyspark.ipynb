{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.10.11\n",
    "import os \n",
    "os.environ['SPARK_HOME']=r'C:/spark/'\n",
    "os.environ['HADOOP_HOME'] = r'C:/hadoop/'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='lab'\n",
    "os.environ['PYSPARK_PYTHON']='python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL_Desastres</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1db7356fdf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, sum\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"ETL_Desastres\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos crudos y dataframes de spark\n",
    "clima_data = [(2023, 22.5, 230.1), (2024, 22.7, 228.6), (2025, 22.9, 227.5), (2026, 23.1, 226.7),\n",
    "             (2027, 23.2, 226.4), (2028, 23.4, 226.2), (2029, 23.6, 226.1), (2030, 23.8, 225.1)]\n",
    "desastres_data = [(2023, 2, 15, 6, 7, 50), (2024, 1, 12, 8, 9, 46), (2025, 3, 16, 5, 6, 47),\n",
    "                  (2026, 4, 12, 10, 13, 52), (2027, 5, 12, 6, 5, 41), (2028, 4, 18, 3, 2, 39),\n",
    "                  (2029, 2, 19, 5, 6, 49), (2030, 4, 20, 6, 7, 50)]\n",
    "muertes_data = [(2023, 1000, 1300, 1200, 1150, 1500), (2024, 1200, 1250, 1260, 1678, 1940),\n",
    "                (2025, 987, 1130, 1160, 1245, 1200), (2026, 1560, 1578, 1856, 1988, 1245),\n",
    "                (2027, 1002, 943, 1345, 1232, 986), (2028, 957, 987, 1856, 1567, 1756),\n",
    "                (2029, 1285, 1376, 1465, 1432, 1236), (2030, 1145, 1456, 1345, 1654, 1877)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clima_schema = StructType([\n",
    "    StructField(\"año\", IntegerType(), True),\n",
    "    StructField(\"Temperatura\", FloatType(), True),\n",
    "    StructField(\"Oxigeno\", FloatType(), True)\n",
    "])\n",
    "\n",
    "desastres_schema = StructType([\n",
    "    StructField(\"año\", IntegerType(), True),\n",
    "    StructField(\"Tsunamis\", IntegerType(), True),\n",
    "    StructField(\"Olas_Calor\", IntegerType(), True),\n",
    "    StructField(\"Terremotos\", IntegerType(), True),\n",
    "    StructField(\"Erupciones\", IntegerType(), True),\n",
    "    StructField(\"Incendios\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "muertes_schema = StructType([\n",
    "    StructField(\"año\", IntegerType(), True),\n",
    "    StructField(\"R_Menor15\", IntegerType(), True),\n",
    "    StructField(\"R_15_a_30\", IntegerType(), True),\n",
    "    StructField(\"R_30_a_45\", IntegerType(), True),\n",
    "    StructField(\"R_45_a_60\", IntegerType(), True),\n",
    "    StructField(\"R_M_a_60\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "clima_df = spark.createDataFrame(clima_data, clima_schema)\n",
    "desastres_df = spark.createDataFrame(desastres_data, desastres_schema)\n",
    "muertes_df = spark.createDataFrame(muertes_data, muertes_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------+---------+\n",
      "| año|Temperatura|Oxigeno|Cuatrenio|\n",
      "+----+-----------+-------+---------+\n",
      "|2023|       22.5|  230.1|2023-2026|\n",
      "|2024|       22.7|  228.6|2023-2026|\n",
      "|2025|       22.9|  227.5|2023-2026|\n",
      "|2026|       23.1|  226.7|2027-2030|\n",
      "|2027|       23.2|  226.4|2027-2030|\n",
      "|2028|       23.4|  226.2|2027-2030|\n",
      "|2029|       23.6|  226.1|2027-2030|\n",
      "|2030|       23.8|  225.1|2027-2030|\n",
      "+----+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "clima_df = clima_df.withColumn(\"Cuatrenio\", when(clima_df[\"año\"] < 2026, \"2023-2026\").otherwise(\"2027-2030\"))\n",
    "clima_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+----------+-----------+------------+------------+-----------+-------------+------------------+------------------+\n",
      "|Cuatrenio|          Temp_AVG|           Oxi_AVG|T_Tsunamis|T_OlasCalor|T_Terremotos|T_Erupciones|T_Incendios|M_Jovenes_AVG|      M_Adutos_AVG|    M_Ancianos_AVG|\n",
      "+---------+------------------+------------------+----------+-----------+------------+------------+-----------+-------------+------------------+------------------+\n",
      "|2027-2030|23.420000076293945|             226.1|        19|         81|          30|          33|        231|       2457.8|            3148.0|            1420.0|\n",
      "|2023-2026|22.700000127156574|228.73333740234375|         6|         43|          19|          22|        143|       2289.0|2564.3333333333335|1546.6666666666667|\n",
      "+---------+------------------+------------------+----------+-----------+------------+------------+-----------+-------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregacion final\n",
    "final_result = clima_df.join(desastres_df, \"año\").join(muertes_df, \"año\") \\\n",
    "    .groupBy(\"Cuatrenio\") \\\n",
    "    .agg(avg(\"Temperatura\").alias(\"Temp_AVG\"),\n",
    "         avg(\"Oxigeno\").alias(\"Oxi_AVG\"),\n",
    "         sum(\"Tsunamis\").alias(\"T_Tsunamis\"),\n",
    "         sum(\"Olas_Calor\").alias(\"T_OlasCalor\"),\n",
    "         sum(\"Terremotos\").alias(\"T_Terremotos\"),\n",
    "         sum(\"Erupciones\").alias(\"T_Erupciones\"),\n",
    "         sum(\"Incendios\").alias(\"T_Incendios\"),\n",
    "         avg((muertes_df[\"R_Menor15\"] + muertes_df[\"R_15_a_30\"])).alias(\"M_Jovenes_AVG\"),\n",
    "         avg((muertes_df[\"R_30_a_45\"] + muertes_df[\"R_45_a_60\"])).alias(\"M_Adutos_AVG\"),\n",
    "         avg(muertes_df[\"R_M_a_60\"]).alias(\"M_Ancianos_AVG\"))\n",
    "\n",
    "# Mostrar resultados\n",
    "final_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
